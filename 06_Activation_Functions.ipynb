{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lVlfTg-fR5xs"
            },
            "source": [
                "<h1 style=\"font-size:30px;\">Activation Functions<\/h1>\n",
                " \n",
                "In this notebook, we will explore the most commonly used activation functions in deep learning. They are:\n",
                "\n",
                "1. Sigmoid\n",
                "2. ReLU\n",
                "3. Tanh\n",
                "4. ELU\n",
                "5. Softmax\n",
                "\n",
                "First, we will use low-level TensorFlow functions to implement the above activation functions from scratch. Then, will also explore the high-level APIs that TensorFlow provides using the `tf.nn` module.\n",
                "\n",
                "<img src='https:\/\/learnopencv.com\/wp-content\/uploads\/2022\/01\/c4_02_activations_functions.png' width=800 align='center'>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "D_zDH5btcyzm"
            },
            "source": [
                "## Table of Contents\n",
                "* [1 Implementing Sigmoid Activation Function](#1-Implementing-Sigmoid-Activation-Function)\n",
                "* [2 Implementing ReLU Activation Function](#2-Implementing-ReLU-Activation-Function)\n",
                "* [3 Tanh Activation Function](#3-Tanh-Activation-Function)\n",
                "* [4 ELU Activation Function](#4-ELU-Activation-Function)\n",
                "* [5 Softmax Activation Function](#5-Softmax-Activation-Function)\n",
                "* [6 Implementing Activation Functions with TensorFlow APIs](#6-Implementing-Activation-Functions-with-TensorFlow-APIs)\n",
                "    * [6.1 TensorFlow Sigmoid Activation](#6.1-TensorFlow-Sigmoid-Activation)\n",
                "    * [6.2 TensorFlow ReLU Activation Function](#6.2-TensorFlow-ReLU-Activation-Function)\n",
                "    * [6.3 TensorFlow Tanh Activation Function](#6.3-TensorFlow-Tanh-Activation-Function)\n",
                "    * [6.4 TensorFlow ELU Activation Function](#6.4-TensorFlow-ELU-Activation-Function)\n",
                "    * [6.5 TensorFlow Softmax Function](#6.5-TensorFlow-Softmax-Function)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2021-11-14T17:40:47.847314Z",
                    "start_time": "2021-11-14T17:40:47.833299Z"
                },
                "collapsed": true,
                "executionInfo": {
                    "elapsed": 513,
                    "status": "ok",
                    "timestamp": 1642137596855,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "DZCDVuTwR5x3"
            },
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "block_plot = False\n",
                "plt.style.use('ggplot')\n",
                "plt.rcParams[\"figure.figsize\"] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5X0WOCY_2KcZ"
            },
            "source": [
                "We will be visualizing the plot of each activation function output. So, before moving ahead into the implementation of the functions, let's write a small function that will plot these graphs by taking a certain number of inputs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": true,
                "executionInfo": {
                    "elapsed": 6,
                    "status": "ok",
                    "timestamp": 1642137597280,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "_HDITLgGoTxk"
            },
            "outputs": [],
            "source": [
                "def plot_activation(x, out_x, y_label, title):\n",
                "    plt.figure\n",
                "    plt.plot(x, y, color='b')\n",
                "    plt.xlabel('x')\n",
                "    plt.ylabel(y_label)\n",
                "    plt.title(title)\n",
                "    plt.show(block=block_plot)\n",
                "    plt.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0mR0Wu8lR5x1"
            },
            "source": [
                "## 1 Implementing Sigmoid Activation Function"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NYwD_rJZR5x5"
            },
            "source": [
                "The Sigmoid function is given by:\n",
                "\n",
                "$$\n",
                "y' = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
                "$$\n",
                "\n",
                "It is best suited for binary classification. As such, we can infer a few things from the following:\n",
                "\n",
                "- If $sigmoid(z) > 0.5$ then input belongs to the positive class or class `1`\n",
                "- If $sigmoid(z) < 0.5$ then input belongs to the negative class or class `0`\n",
                "\n",
                "The `sigmoid` output $y'$ may be thought of as the probability that the data point belongs to class `1`. So, the probability that it belongs to class `0` will be $1-y'$.\n",
                "\n",
                "It is mostly used in the final layer of a neural network.\n",
                "\n",
                "One of the biggest disadvantages of the Sigmoid activation is the vanishing gradient problem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2021-11-14T17:40:49.246313Z",
                    "start_time": "2021-11-14T17:40:48.982298Z"
                },
                "code_folding": [],
                "collapsed": true,
                "executionInfo": {
                    "elapsed": 5,
                    "status": "ok",
                    "timestamp": 1642137597281,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "LmxeRYH2R5x5"
            },
            "outputs": [],
            "source": [
                "def sigmoid(x):\n",
                "    return 1\/(1 + tf.math.exp(-x))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 640
                },
                "executionInfo": {
                    "elapsed": 1140,
                    "status": "ok",
                    "timestamp": 1642137598417,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "LtmXvCd3jt1s",
                "outputId": "7c741fe4-3284-4336-ae1a-d34c48fe404c"
            },
            "outputs": [],
            "source": [
                "# Plot sigmoid activation function.\n",
                "x = tf.linspace(-10, 10, 1000)\n",
                "y = sigmoid(x)\n",
                "print(y[:10])\n",
                "\n",
                "plot_activation(x, y, 'sigmoid(x)', 'Sigmoid Activation')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "s47R5ulSVwT4"
            },
            "source": [
                "**We can observe the following points in `sigmoid` plot:**\n",
                "\n",
                "- $sigmoid(0) = 0.5$\n",
                "- $sigmoid(x) > 0.5$    $\\text{  }\\forall\\text{ }y > 0$\n",
                "- $sigmoid(x) < 0.5$    $\\text{  }\\forall\\text{ }y < 0$\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "tabM9h0JR5x_"
            },
            "source": [
                "## 2 Implementing ReLU Activation Function\n",
                "For any give input, the ReLU (Rectified Linear Unit) activation function either returns 0 or the same value as the input. The following is the formula:\n",
                "\n",
                "$$\n",
                "ReLU(x) = max(0, x)\n",
                "$$\n",
                "\n",
                "**So, when does it return 0?** *Whenever the input value is less than 0, it returns 0, else always return the same value as the input*.\n",
                "\n",
                "Breaking down the above explanation into a simple $if$ statements will look something like this:\n",
                "$$\n",
                "   ReLU(x) = \n",
                "\\begin{cases}\n",
                "    0,& \\text{if } x < 0\\\\\n",
                "    x,              & \\text{otherwise}\n",
                "\\end{cases}\n",
                "$$\n",
                "Now, let's implement ReLU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "collapsed": true,
                "executionInfo": {
                    "elapsed": 14,
                    "status": "ok",
                    "timestamp": 1642137598417,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "fHmwvELlkFVC"
            },
            "outputs": [],
            "source": [
                "def relu(x):\n",
                "    return tf.math.maximum(0, x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 588
                },
                "executionInfo": {
                    "elapsed": 14,
                    "status": "ok",
                    "timestamp": 1642137598418,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "2KzNCgaImW0o",
                "outputId": "b1189c15-3d17-482f-f6a5-10655a4c40c1"
            },
            "outputs": [],
            "source": [
                "# Plot relu activation function.\n",
                "x = tf.linspace(-10, 10, 1000)\n",
                "y = relu(x)\n",
                "print(y[:10])\n",
                "\n",
                "plot_activation(x, y, 'relu(x)', 'ReLU Activation')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "GSkPnJHIoRG7"
            },
            "source": [
                "## 3 Tanh Activation Function\n",
                "The tanh activation function is somewhat similar to the sigmoid activation function, at least, in terms of the plot.\n",
                "\n",
                "But instead of the output range being between 0 to 1, it ranges from -1 to 1.\n",
                "\n",
                "And the following gives the formula for tanh activation:\n",
                "\n",
                "$$\n",
                "tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
                "$$\n",
                "\n",
                "The following code block shows the implementation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "collapsed": true,
                "executionInfo": {
                    "elapsed": 12,
                    "status": "ok",
                    "timestamp": 1642137598419,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "qlte8lCMrc25"
            },
            "outputs": [],
            "source": [
                "def tanh(x):\n",
                "    return (tf.math.exp(x) - tf.math.exp(-x)) \/ (tf.math.exp(x) + tf.math.exp(-x))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 623
                },
                "executionInfo": {
                    "elapsed": 1331,
                    "status": "ok",
                    "timestamp": 1642137599739,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "glZgbzdOr5xj",
                "outputId": "3498eb4c-6579-46f8-b932-a8dfd715b38a"
            },
            "outputs": [],
            "source": [
                "# Plot tanh activation function.\n",
                "x = tf.linspace(-10, 10, 1000)\n",
                "y = tanh(x)\n",
                "print(y[:10])\n",
                "\n",
                "plot_activation(x, y, 'tanh(x)', 'Tanh Activation')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_k10CvAbIvEu"
            },
            "source": [
                "## 4 ELU Activation Function\n",
                "ELU stand for Exponential Linear Unit. Compared to ReLU, this also has an alpha constant ($\\alpha$) which also applies a bit of non-linearity to when the values are negative.\n",
                "\n",
                "We can calculate ELU as:\n",
                "$$\n",
                "    ELU(x)= \n",
                "\\begin{cases}\n",
                "    \\alpha(e^x - 1),& \\text{if } x < 0\\\\\n",
                "    x,              & \\text{otherwise}\n",
                "\\end{cases}\n",
                "$$\n",
                "\n",
                "It has a few advantanges over ReLU such as:\n",
                "* ELU is less likely to have an exploding gradient problem as sometimes maybe the case with ReLU.\n",
                "* Also, ELU does not suffer from the *dying relu* issue as ReLU does.\n",
                "\n",
                "Still, one major disadvantage of ELU is that it is slower to compute because it also applies non-linearity to negative inputs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "collapsed": true,
                "executionInfo": {
                    "elapsed": 50,
                    "status": "ok",
                    "timestamp": 1642137599740,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "xsFMVpNdIylq"
            },
            "outputs": [],
            "source": [
                "def elu(x):\n",
                "    alpha = 1.0\n",
                "    return [i if i > 0 else alpha*tf.math.exp(i)-1 for i in x]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 608
                },
                "executionInfo": {
                    "elapsed": 48,
                    "status": "ok",
                    "timestamp": 1642137599741,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "D34glq3JLJHk",
                "outputId": "967f8aa3-09cd-4e80-a1fd-a2e32e7a54c8"
            },
            "outputs": [],
            "source": [
                "# Plot elu activation function.\n",
                "x = tf.linspace(-10, 10, 1000)\n",
                "y = elu(x)\n",
                "print(y[:10])\n",
                "\n",
                "plot_activation(x, y, 'elu(x)', 'ELU Activation')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "99thQPrDG0OQ"
            },
            "source": [
                "## 5 Softmax Activation Function\n",
                "Although we can call Softmax an activation function, *but it is not really an activation function*.\n",
                "\n",
                "The Softmax function returns the probability distribution of the give inputs. Typically, the inputs can be values from -\u221e to +\u221e. But after the Softmax activation has been applied, the output values are from 0 to 1.\n",
                "\n",
                "And it is mostly used in the last layer of neural network. The Softmax function takes in the logits (direct outputs of the neural network) and converts them to probability distribution for further operations.\n",
                "\n",
                "Let's take a look at the formula:\n",
                "$$\n",
                "Softmax(x_i) = \\frac{exp(x_i)}{\\sum_{i=0}^{n}exp(x_i)}\n",
                "$$\n",
                "\n",
                "Now implementing the same using Python."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "collapsed": true,
                "executionInfo": {
                    "elapsed": 37,
                    "status": "ok",
                    "timestamp": 1642137599742,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "gpFkdCeIG5v5"
            },
            "outputs": [],
            "source": [
                "def softmax(x):\n",
                "    return (tf.math.exp(x) \/ tf.reduce_sum(tf.math.exp(x)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "U_guP8CBbH9u"
            },
            "source": [
                "Now, let's check out how softmax may actually change the values from simple model outputs (logits) to probability distribution.\n",
                "\n",
                "<img src='https:\/\/learnopencv.com\/wp-content\/uploads\/2022\/01\/c4_02_softmax_numeric_conversion.png' width=600 align='center'>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/"
                },
                "executionInfo": {
                    "elapsed": 35,
                    "status": "ok",
                    "timestamp": 1642137599742,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "TidyWNIHG73T",
                "outputId": "97728fee-f1b0-4f4a-d8ca-f0b9695be431"
            },
            "outputs": [],
            "source": [
                "# Plot softmax activation function.\n",
                "x = tf.constant([-3, 5, 1], dtype=tf.float32)\n",
                "y = softmax(x)\n",
                "print(y.numpy())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TYovy3g5vJLL"
            },
            "source": [
                "## 6 Implementing Activation Functions with TensorFlow APIs\n",
                "We can also implement all of the above activation functions using the `tf.nn` module. We need not write the function on our own. Instead, we just need to call the right function with one line of code."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "q1h52YvZxowD"
            },
            "source": [
                "### 6.1 TensorFlow Sigmoid Activation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 640
                },
                "executionInfo": {
                    "elapsed": 30,
                    "status": "ok",
                    "timestamp": 1642137599743,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "tsYAEa2Y0a59",
                "outputId": "8f54b769-9ffa-4ed4-a8f2-196b738c1225"
            },
            "outputs": [],
            "source": [
                "# Plot sigmoid activation function.\n",
                "x = tf.linspace(-10, 10, 1000)\n",
                "y = tf.nn.sigmoid(x)\n",
                "print(y[:10])\n",
                "\n",
                "plot_activation(x, y, 'sigmoid(x)', 'Sigmoid Activation')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1oQSI5zZ0ds-"
            },
            "source": [
                "### 6.2 TensorFlow ReLU Activation Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 588
                },
                "executionInfo": {
                    "elapsed": 24,
                    "status": "ok",
                    "timestamp": 1642137599744,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "ps1Qsdnj0ub6",
                "outputId": "dff5a846-7191-48b5-81a6-e06d3908093e"
            },
            "outputs": [],
            "source": [
                "# Plot relu activation function.\n",
                "x = tf.linspace(-10, 10, 1000)\n",
                "y = tf.nn.relu(x)\n",
                "print(y[:10])\n",
                "\n",
                "plot_activation(x, y, 'relu(x)', 'ReLU Activation')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "utUnyxyX0yoM"
            },
            "source": [
                "### 6.3 TensorFlow Tanh Activation Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 623
                },
                "executionInfo": {
                    "elapsed": 1346,
                    "status": "ok",
                    "timestamp": 1642137601070,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "-me9h2iX056O",
                "outputId": "f7b63ebd-a2ad-45ea-dddb-0d05aa519fc6"
            },
            "outputs": [],
            "source": [
                "# Plot tanh activation function.\n",
                "x = tf.linspace(-10, 10, 1000)\n",
                "y = tf.nn.tanh(x)\n",
                "print(y[:10])\n",
                "\n",
                "plot_activation(x, y, 'tanh(x)', 'Tanh Activation')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0OuTL9nuzICh"
            },
            "source": [
                "### 6.4 TensorFlow ELU Activation Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/",
                    "height": 623
                },
                "executionInfo": {
                    "elapsed": 49,
                    "status": "ok",
                    "timestamp": 1642137601070,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "Di5F5xbCzNMA",
                "outputId": "03e0e7d3-fc0b-4e31-ee00-7eb18be5e86a"
            },
            "outputs": [],
            "source": [
                "# Plot elu activation function.\n",
                "x = tf.linspace(-10, 10, 1000)\n",
                "y = tf.nn.elu(x)\n",
                "print(y[:10])\n",
                "\n",
                "plot_activation(x, y, 'elu(x)', 'ELU Activation')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "joyJZ87p0-AF"
            },
            "source": [
                "### 6.5 TensorFlow Softmax Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/"
                },
                "executionInfo": {
                    "elapsed": 36,
                    "status": "ok",
                    "timestamp": 1642137601071,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "RdutRiT11DCI",
                "outputId": "614a42e2-af43-454a-e69c-e737b38419cf"
            },
            "outputs": [],
            "source": [
                "# Plot softmax activation function.\n",
                "x = tf.constant([-3, 5, 1], dtype=tf.float32)\n",
                "y = tf.nn.softmax(x)\n",
                "print(y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "wF1I5zv-1Hkj"
            },
            "source": [
                "As you can see from the above plots, the values as well as the graphs as exactly the same as we had in the manual implementation case. But here, we calling the activation functions using the `tf.nn` module with just one line of code. This is much more convient while doing large projects, as it is less error-prone and makes the code more concise as well."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "-DG4i6iE1p3O"
            },
            "source": [
                "## 7 Time Comparison Between Different Activation Functions\n",
                "\n",
                "Let's check out the execution time of each activation function here. We will use the `tf.nn` module to check run them again.\n",
                "\n",
                "We will take the average of 50 runs for each of the activation functions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "collapsed": true,
                "executionInfo": {
                    "elapsed": 27,
                    "status": "ok",
                    "timestamp": 1642137601071,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "HiHL1rO851W8"
            },
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "# List to store all the activation function execution times.\n",
                "activation_times = []\n",
                "N_RUNS = 50"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "collapsed": true,
                "executionInfo": {
                    "elapsed": 27,
                    "status": "ok",
                    "timestamp": 1642137601072,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "2ogH9oZV7ckR"
            },
            "outputs": [],
            "source": [
                "def calculate_and_store_time(\n",
                "    function_name, avg_run_time\n",
                "):\n",
                "    print(f\"Time taken for {function_name}: {avg_run_time:.3f}\")\n",
                "    activation_times.append(avg_run_time)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/"
                },
                "executionInfo": {
                    "elapsed": 26,
                    "status": "ok",
                    "timestamp": 1642137601072,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "T3eQpLH-5asw",
                "outputId": "499811f3-9972-4515-e4b7-da88eab1fb98"
            },
            "outputs": [],
            "source": [
                "run_times = []\n",
                "for i in range(N_RUNS):\n",
                "    start = time.time()\n",
                "    x = tf.linspace(-100, 100, 100000)\n",
                "    y = tf.nn.sigmoid(x)\n",
                "    end = time.time()\n",
                "    run_times.append(end-start)\n",
                "\n",
                "avg_run_time = sum(run_times)\/len(run_times)\n",
                "calculate_and_store_time('sigmoid', avg_run_time)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/"
                },
                "executionInfo": {
                    "elapsed": 22,
                    "status": "ok",
                    "timestamp": 1642137601073,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "kpWLkA957U-K",
                "outputId": "960b6ac5-d6b0-4156-d461-91a7661f3e64"
            },
            "outputs": [],
            "source": [
                "run_times = []\n",
                "for i in range(N_RUNS):\n",
                "    start = time.time()\n",
                "    x = tf.linspace(-100, 100, 100000)\n",
                "    y = tf.nn.relu(x)\n",
                "    end = time.time()\n",
                "    run_times.append(end-start)\n",
                "\n",
                "avg_run_time = sum(run_times)\/len(run_times)\n",
                "calculate_and_store_time('relu', avg_run_time)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/"
                },
                "executionInfo": {
                    "elapsed": 19,
                    "status": "ok",
                    "timestamp": 1642137601073,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "JheSwY628VmS",
                "outputId": "6b9c0e37-5f77-4d4b-d72e-8e37239641b9"
            },
            "outputs": [],
            "source": [
                "run_times = []\n",
                "for i in range(N_RUNS):\n",
                "    start = time.time()\n",
                "    x = tf.linspace(-100, 100, 100000)\n",
                "    y = tf.nn.tanh(x)\n",
                "    end = time.time()\n",
                "    run_times.append(end-start)\n",
                "\n",
                "avg_run_time = sum(run_times)\/len(run_times)\n",
                "calculate_and_store_time('tanh', avg_run_time)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/"
                },
                "executionInfo": {
                    "elapsed": 17,
                    "status": "ok",
                    "timestamp": 1642137601074,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "L3NuIUA88aRM",
                "outputId": "35386234-6507-4a91-ea95-a64995d75482"
            },
            "outputs": [],
            "source": [
                "run_times = []\n",
                "for i in range(N_RUNS):\n",
                "    start = time.time()\n",
                "    x = tf.linspace(-100, 100, 100000)\n",
                "    y = tf.nn.elu(x)\n",
                "    end = time.time()\n",
                "    run_times.append(end-start)\n",
                "\n",
                "avg_run_time = sum(run_times)\/len(run_times)\n",
                "calculate_and_store_time('elu', avg_run_time)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/"
                },
                "executionInfo": {
                    "elapsed": 658,
                    "status": "ok",
                    "timestamp": 1642137601718,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "uhFkFwq98gHJ",
                "outputId": "0b800ada-6950-432e-ef07-c808729ac222"
            },
            "outputs": [],
            "source": [
                "run_times = []\n",
                "for i in range(N_RUNS):\n",
                "    start = time.time()\n",
                "    x = tf.linspace(-100, 100, 100000)\n",
                "    y = tf.nn.softmax(x)\n",
                "    end = time.time()\n",
                "    run_times.append(end-start)\n",
                "\n",
                "avg_run_time = sum(run_times)\/len(run_times)\n",
                "calculate_and_store_time('softmax', avg_run_time)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "colab": {
                    "base_uri": "https:\/\/localhost:8080\/"
                },
                "executionInfo": {
                    "elapsed": 414,
                    "status": "ok",
                    "timestamp": 1642137623326,
                    "user": {
                        "displayName": "Sovit Rath",
                        "photoUrl": "https:\/\/lh3.googleusercontent.com\/a\/default-user=s64",
                        "userId": "15729028473288306186"
                    },
                    "user_tz": -330
                },
                "id": "SQ7yfxiV8mdg",
                "outputId": "05134a0c-3544-425a-9a3c-5688ed1e90af"
            },
            "outputs": [],
            "source": [
                "print(activation_times)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Fazlj8yPB5PZ"
            },
            "source": [
                "As you can see, although by a small margin, elu and tanh are taking the most amount of time."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "oCsDPdSdEdKp"
            },
            "source": [
                "<img src='https:\/\/learnopencv.com\/wp-content\/uploads\/2022\/01\/c4_02_activation_function_times.png' width=600 align='center'>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "id": "yWSRHYp_B9zm"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "colab": {
            "collapsed_sections": [],
            "name": "week_02_09b_Activation_Functions.ipynb",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3.8",
            "language": "python",
            "name": "python38"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.7"
        },
        "latex_envs": {
            "LaTeX_envs_menu_present": true,
            "autoclose": false,
            "autocomplete": true,
            "bibliofile": "biblio.bib",
            "cite_by": "apalike",
            "current_citInitial": 1,
            "eqLabelWithNumbers": true,
            "eqNumInitial": 1,
            "hotkeys": {
                "equation": "Ctrl-E",
                "itemize": "Ctrl-I"
            },
            "labels_anchors": false,
            "latex_user_defs": false,
            "report_style_numbering": false,
            "user_envs_cfg": false
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": [],
            "number_sections": false,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {
                "height": "calc(100% - 180px)",
                "left": "10px",
                "top": "150px",
                "width": "325.198px"
            },
            "toc_section_display": false,
            "toc_window_display": false
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "position": {
                "height": "415.828px",
                "left": "909.036px",
                "right": "20px",
                "top": "98.9838px",
                "width": "326.98px"
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}