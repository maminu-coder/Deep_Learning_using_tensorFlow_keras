{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUqhq-Z9eAVd"
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Pre-Trained Models in Keras Applications</h1>\n",
    "\n",
    "Many well-known CNN models have been **pre-trained** on large benchmark datasets like **ImageNet**. The Deep Learning community has greatly benefited from these open-source models. Also, the pre-trained models are a major factor for rapid advances in Computer Vision research. Other researchers and practitioners can use these state-of-the-art models instead of re-inventing everything from scratch.\n",
    "\n",
    "<img src='https://opencv.org/wp-content/uploads/2022/03/c4_03_VGG-16_pretrained.png' width=1000 align='center'><br/>\n",
    "\n",
    "### What is ImageNet?\n",
    "[ImageNet](http://www.image-net.org/) is a project which aims to provide a large image database for research purposes. It contains more than 1.2 million images comprising 1,000 classes (or synsets). They also provide bounding box annotations which can be used in Object Localization tasks. It should be noted that they only provide urls of images, and you need to download those images. For reference, here is the list of class labels for ImageNet <a href=\"https://image-net.org/challenges/LSVRC/2012/browse-synsets\" target=\"_blank\">ImageNet Class Labels.</a> \n",
    "\n",
    "### How Can We Use Pre-Trained Models?\n",
    "We can use them right out-of-the-box for image classification or we can use them as a starting point for further training to fine-tune the for own custom dataset. Many of these State-of-the-Art models are already available through TensorFlow in the form of [Keras Applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications) or from [Tensorflow Hub](https://tfhub.dev/) which have been pre-trained on the ImageNet dataset. So, we can use them directly for many types of image classification tasks as long as the image classes in our dataset are also represented in the ImageNet dataset. \n",
    "\n",
    "In this Notebook, we will take a look at several models available in the Keras Applications API and explore how to use these models on our own images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T08:51:43.994788Z",
     "start_time": "2022-02-03T08:51:43.975790Z"
    },
    "id": "NiV0GiSfeAVh"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1 Pre-trained Classification Models in TensorFlow](#1-Pre\\-Trained-Classification-Models-in-TensorFlow)\n",
    "* [2 Using a Pre-Trained Model](#2-Using-a-Pre\\-Trained-Model)\n",
    "* [3 Model Comparison](#3-Model-Comparison)\n",
    "* [4 Conclusion](#3-Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:15:57.593900Z",
     "start_time": "2021-12-29T19:15:53.015598Z"
    },
    "collapsed": true,
    "id": "we0mCvmQSz5c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "block_plot = False\n",
    "\n",
    "# Text formatting.\n",
    "bold = \"\\033[1m\"\n",
    "end = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1xjoDxFS4Jg"
   },
   "source": [
    "## 1 Pre-Trained Classification Models in TensorFlow\n",
    "\n",
    "TensorFlow provides a host of pre-trained models through its <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications\" target=\"_blank\">`tf.keras.applications`</a> API. It contains image classification models that have been pre-trained on the ImageNet dataset. Let's now take a look at all the models available in TensorFlow's `keras.applications`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:15:57.749002Z",
     "start_time": "2021-12-29T19:15:57.720028Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gM3LKa7qS2SW",
    "outputId": "03e3891a-b354-4328-cd2b-c29ad8278e08",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_modules = dir(tf.keras.applications)\n",
    "\n",
    "# Navigate the tf.keras.applications namespace and display the available models.\n",
    "for module in all_modules:\n",
    "    if module[0].islower():\n",
    "        if module != 'imagenet_utils':\n",
    "            # Ignore imagenet_utils.\n",
    "            print(f\"Model Family: {bold}{module}{end}\")\n",
    "        if module == \"mobilenet_v3\":\n",
    "            # Handel special case for mobilenet_v3.\n",
    "            temp = \"MobileNetV3Large\"\n",
    "            print(f\"\\t  |__ {temp}\")\n",
    "            temp = \"MobileNetV3LSmall\"\n",
    "            print(f\"\\t  |__ {temp}\")\n",
    "        else:\n",
    "            # General case.\n",
    "            models = dir(getattr(tf.keras.applications, module))\n",
    "            for model in models:\n",
    "                if model[0].isupper():\n",
    "                    print(f\"\\t  |__ {model}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mzLK_3GTACV"
   },
   "source": [
    "## 2 Using a Pre-Trained Model\n",
    "To use any of the pre-trained models listed above, we need to follow a few simple steps:\n",
    "\n",
    "1. Select a pre-trained model and specify the input size required by the model\n",
    "2. Load a test image and resize it to the appropriate size based on the selected model\n",
    "3. Pre-process the input image(s) using a dedicated pre-processing function that is accessible in the model, `preprocess_input()`\n",
    "4. Call the model's `predict()` method to generate predictions\n",
    "5. De-code the predictions using a dedicated post-processing function that is accessible in the model, `decode_predictions()`\n",
    "\n",
    "### 2.1 Load and Display a Test Image\n",
    "\n",
    "Let's first load and display a test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:15:57.717138Z",
     "start_time": "2021-12-29T19:15:57.596901Z"
    },
    "collapsed": true,
    "id": "d_sEwfFHE5k3"
   },
   "outputs": [],
   "source": [
    "IMG_PATH = r\"./test.jpg\"\n",
    "\n",
    "# Download a sample test image.\n",
    "url = 'https://www.dropbox.com/scl/fi/trn5tqm7vhcwwzdbvyoi5/panda_test_image_720.jpg?rlkey=lgfpemkiikowbem9gdk6u6ve3&dl=1' #'https://learnopencv.com/wp-content/uploads/2021/10/panda_test_image.jpg'\n",
    "\n",
    "with open(IMG_PATH, 'wb') as f:\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:16:01.658470Z",
     "start_time": "2021-12-29T19:16:01.439479Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "collapsed": true,
    "id": "CCBCMfZW2Upn",
    "outputId": "2b0fe869-dc33-404c-8afa-2cb75e7ce8f8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read and display the sample image.\n",
    "image = plt.imread(IMG_PATH)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show(block=block_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc4W2Jgy07xX"
   },
   "source": [
    "### 2.2 Select a Model and Specify the Required Input Size for the Model\n",
    "\n",
    "In the code cell below we have provided a list of all the models available in the Keras Applications API along with the corresponding input image sizes required for each model. You can use this notebook to execute any of these models by simply uncommenting the appropriate `model_family` name and the `model_name` that you would like to use. Many of the models require an input shape of `(224, 224, 3)` but there are also several models that require a different input shape, so you will also need to specify the appropriate input image size, `IMG_SHAPE`, depending on the model you select.\n",
    "\n",
    "Let's try classifying the image of the panda using the `MobileNetV3Small` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "LToiSfYN07xX"
   },
   "outputs": [],
   "source": [
    "# Specify the input image size based on the selected model.\n",
    "IMG_SHAPE = (224, 224, 3) \n",
    "\n",
    "# model_family = \"densenet\"\n",
    "# model_name = \"DenseNet121\"           # (224, 224, 3)\n",
    "# model_name = \"DenseNet169\"           # (224, 224, 3)\n",
    "# model_name = \"DenseNet201\"           # (224, 224, 3)\n",
    "\n",
    "# model_family = \"EfficientNet\"\n",
    "# model_name   = \"EfficientNetB0\"      # (224, 224, 3)\n",
    "# model_name   = \"EfficientNetB1\"      # (240, 240, 3)\n",
    "# model_name   = \"EfficientNetB2\"      # (260, 260, 3)\n",
    "# model_name   = \"EfficientNetB3\"      # (300, 300, 3)\n",
    "# model_name   = \"EfficientNetB4\"      # (380, 380, 3)\n",
    "# model_name   = \"EfficientNetB5\"      # (456, 456, 3)\n",
    "# model_name   = \"EfficientNetB6\"      # (528, 528, 3)\n",
    "# model_name   = \"EfficientNetB7\"      # (600, 600, 3)\n",
    "\n",
    "# model_family = \"inception_resnet_v2\"\n",
    "# model_name   = \"InceptionResNetV2\"   # (299, 299, 3)\n",
    "\n",
    "# model_family = \"inception_v3\"     \n",
    "# model_name   = \"InceptionV3\"         # (299, 299, 3)\n",
    "\n",
    "# model_family = \"mobilenet\"           \n",
    "# model_name   = \"MobileNet\"           # (224, 224, 3)\n",
    "\n",
    "# model_family = \"mobilenet_v2\"\n",
    "# model_name   = \"MobileNetV2\"         # (224, 224, 3)\n",
    "\n",
    "# model_family = \"mobilenet_v3\"\n",
    "# model_name   = \"MobileNetV3\"         # (224, 224, 3)\n",
    "\n",
    "model_family = \"mobilenet_v3\"\n",
    "model_name   = \"MobileNetV3Small\"      # (224, 224, 3)\n",
    "# model_name   = \"MobileNetV3Large\"    # (224, 224, 3)\n",
    "\n",
    "# model_family = \"nasnet\"\n",
    "# model_name   = \"NASNetMobile\"        # (224, 224, 3)\n",
    "\n",
    "# model_family = \"resnet\"             \n",
    "# model_name   = \"ResNet101\"           # (224, 224, 3)\n",
    "# model_name   = \"ResNet152\"           # (224, 224, 3)\n",
    "# model_name   = \"ResNet50\"            # (224, 224, 3)\n",
    "\n",
    "# model_family = \"resnet50\"\n",
    "# model_name   = \"ResNet50\"            # (224, 224, 3)\n",
    "\n",
    "# model_family = \"resnet_v2\"\n",
    "# model_name   = \"ResNet101V2\"         # (224, 224, 3)\n",
    "# model_name   = \"ResNet152V2\"         # (224, 224, 3)\n",
    "# model_name   = \"ResNet50V2\"          # (224, 224, 3)\n",
    "\n",
    "# model_family = \"vgg16\"\n",
    "# model_name   = \"VGG16\"               # (224, 224, 3)\n",
    "\n",
    "# model_family = \"vgg19\"\n",
    "# model_name   = \"VGG19\"               # (224, 224, 3)\n",
    "\n",
    "# model_family = \"xception\"\n",
    "# model_name   = \"Xception\"            # (299, 299, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8V9oQcc07xY"
   },
   "source": [
    "### 2.3 Loading an Image\n",
    "\n",
    "In this section we develop a convenience function to load and resize the input image. Later in this module we will introduce a utility in Keras that allows you to easily process multiple images (`image_dataset_from_directory()`). For now, we will just use the function below to read and pre-process a single image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:16:01.436470Z",
     "start_time": "2021-12-29T19:16:01.424479Z"
    },
    "collapsed": true,
    "id": "lljo6hAzW_4d"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, img_shape):\n",
    "    \n",
    "    # Load the image.\n",
    "    image = tf.io.read_file(IMG_PATH)\n",
    "    \n",
    "    # Convert the image from bytes to an image tensor.\n",
    "    x = tf.image.decode_image(image, channels=img_shape[2])\n",
    "    \n",
    "    # Resize image to the input shape required by the model.\n",
    "    x = tf.image.resize(x, (img_shape[0], img_shape[1]))\n",
    "    \n",
    "    # Add a dimension for an image batch representation.\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "KmislJY707xZ",
    "outputId": "fc0c9a43-52c8-4275-bfd1-d46911191d22"
   },
   "outputs": [],
   "source": [
    "# Load the image and re-shape the image tensor.\n",
    "input = load_image(image_path=IMG_PATH, img_shape=IMG_SHAPE)\n",
    "print(\"Shape: \", input.shape)\n",
    "print(\"Data type: \", input.dtype)\n",
    "print(\"Min pixel value: \", tf.math.reduce_min(input).numpy())\n",
    "print(\"Max pixel value: \", tf.math.reduce_max(input).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLubsFIY07xZ"
   },
   "source": [
    "### 2.4 Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:15:57.765000Z",
     "start_time": "2021-12-29T19:15:57.754010Z"
    },
    "collapsed": true,
    "id": "OsuM3qwJxJ8Y"
   },
   "outputs": [],
   "source": [
    "# This is a convenience function that loads any model available in the `tf.keras.applications`.\n",
    "def load_model(input_shape, model_name):  \n",
    "    \n",
    "    # Models will be loaded wth pre-trainied `imagenet` weights.\n",
    "    model = getattr(tf.keras.applications, model_name)(input_shape=input_shape, weights=\"imagenet\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2ozEszWxOSC"
   },
   "source": [
    "### 2.5 Model Pre-Processing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC7P9_HU4VI0"
   },
   "source": [
    "Before an input image is passed to the model, it must first be pre-processed. Each model available in `tf.keras.applications` has a dedicated pre-processing function called `preprocess_input()`. This function is available in each model family's namespace. For example, the VGG-16 pre-processing function is accessed via the namespace: <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input\" target=\"_blank\">`tf.keras.applications.vgg16.preprocess_input`</a>. The function syntax is shown below, where `x` represents a floating point numpy.array or a tf.Tensor. Refer to the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications\" target=\"_blank\">`documentation`</a> for each model to learn more about the processing performed by `preprocess_input()`. \n",
    "\n",
    "```python\n",
    "preprocess_input(x, data_format=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAoEP0_u07xb"
   },
   "source": [
    "### 2.6 Model Decoding Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tmejf9jJctvZ"
   },
   "source": [
    "Each model also comes with a helper function to decode the raw predictions from an ImageNet model to a human-readable format. This function is also accessed via the model's namespace. The first argument is a Numpy array encoding a batch of predictions (the output from the model's `predict()` function). The second argument is the number of predictions to decode, ranked by the highest probability score.\n",
    "\n",
    "```python\n",
    "predictions = decode_predictions(preds, top=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_xgoCHlctva"
   },
   "source": [
    "### 2.7 Processing Pipeline Function\n",
    "\n",
    "In this section, we will now develop a convenience function that performs the following processing steps:\n",
    "\n",
    "- Pre-process an input image using the model's `preprocess_input()` function\n",
    "- Call the model's `predict()` method\n",
    "- Decode the predictions using the model's `decode_predictions()` function\n",
    "\n",
    "Each model generates the class probabilities of all 1,000 classes, but we're mainly interested in the highest-scoring class. However, it can be very informative to inspect the `top_k` classes to get a better sense for the model's robustness. So the optional argument `top_k` is included so that we can control the number of predictions produced by the model's `decode_predictions()` function. For example, if `top_k=5`, this means the function will show the top 5 predictions based on the top 5 probability scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:16:01.674476Z",
     "start_time": "2021-12-29T19:16:01.662491Z"
    },
    "collapsed": true,
    "id": "87c4A6f2xMA4"
   },
   "outputs": [],
   "source": [
    "def generate_predictions(input, model_family, model, top_k=3):\n",
    "    \n",
    "    # Initialize pre and post processing functions for the chosen model_family.\n",
    "    preprocess  = getattr(tf.keras.applications, model_family).preprocess_input\n",
    "    postprocess = getattr(tf.keras.applications, model_family).decode_predictions\n",
    "\n",
    "    # Pre-process the input image.\n",
    "    x = preprocess(input)\n",
    "\n",
    "    # Generate predictions.\n",
    "    preds = model.predict(x)\n",
    "\n",
    "    # Print top_k predictions.\n",
    "    post_preds = postprocess(preds, top=top_k)[0]\n",
    "\n",
    "    print('Predicted')\n",
    "\n",
    "    for i in post_preds:\n",
    "        print(f\"Class Description: {bold}{i[1]:<30}{end} Score: {bold}{i[2]}{end}\")\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIbFqmGhrhxr"
   },
   "source": [
    "### 2.8 Perform Inference\n",
    "\n",
    "To perform inference using the selected model, we will use the three functions we developed above.\n",
    "\n",
    "1. `load_image()`\n",
    "2. `load_model()`\n",
    "3. `generate_predictions()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:16:09.601146Z",
     "start_time": "2021-12-29T19:16:01.679476Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9RypoPmQXOL-",
    "outputId": "64a60ee3-1c3c-4f09-f54d-ab9e34cfdc4c"
   },
   "outputs": [],
   "source": [
    "# Load an image.\n",
    "input = load_image(image_path=IMG_PATH, img_shape=IMG_SHAPE)\n",
    "\n",
    "# Load the model.\n",
    "pretrained_model = load_model(input_shape=IMG_SHAPE, model_name=model_name)\n",
    "\n",
    "# Generate predictions.\n",
    "predictions = generate_predictions(input=input,\n",
    "                                       model_family=model_family,\n",
    "                                       model=pretrained_model,\n",
    "                                       top_k=5,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uurelAJieAVt"
   },
   "source": [
    "A shown in the results above, the **MobileNetV3Small** model is able to classify the image correctly as `giant_panda` with a 85% probability. As you can see, the next two predictions also make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edhMw1sdE5k-"
   },
   "source": [
    "## 3 Model Comparison\n",
    "\n",
    "As demonstrated above, using pre-trained models for image classification is very straightforward. Still, with so many models available, it's informative to compare the models based on various performance metrics. Besides model accuracy, inference speed and model size may also be very important considerations depending on the type of device the model will be running on and the type of application. In the following sections, we will compare models based on the following metrics.\n",
    "\n",
    "1. **Top-1 Accuracy**:  The model answer (the one with highest probability) must be exactly the expected answer.\n",
    "2. **Top-5 Accuracy**: Any of the model's top 5 highest probability answers must match the expected answer.\n",
    "3. **Inference Time on CPU**: Inference time is the time taken for model inference step on CPU.\n",
    "4. **Inference Time on GPU**: Inference time is the time taken for model inference step on GPU.\n",
    "5. **Model size**: Here, size stands for the physical space occupied by the model weights file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tip_v1TjE5k_"
   },
   "source": [
    "### 3.1 Accuracy Comparison of Models\n",
    "\n",
    "**Top-1 Accuracy:** When the predicted top ranking class (the one with the highest probability) is the same as the ground-truth class. \n",
    "\n",
    "**Top-5 Accuracy:** A prediction is classified as correct, if **any** of the top-5 predicted classes matches the ground-truth class. \n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2021/10/c4_pretrained_acc_compare.png' width=1000 align='center'><br/>\n",
    "\n",
    "* Note how both metrics follow a similar trend. \n",
    "* The best model with respect to Top-1 and Top-5 accuracy is **`EfficientNet-B7`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vywGQD39E5k_"
   },
   "source": [
    "### 3.2 Inference Time Comparison\n",
    "\n",
    "Next, we compare the models based on the time taken to perform inference (for both CPUs and GPUs). Time per inference step is computed based on the average of 30 batches and 10 repetitions. The data in the plots below were generated using the following:\n",
    "\n",
    "* **Batch size**: 32\n",
    "* **CPU**: AMD EPYC Processor (with IBPB) (92 core) \n",
    "* **GPU**: Tesla A100\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2021/10/c4_pretrained_inf_speed_cpu.png' width=1000 align='center'><br/>\n",
    "\n",
    "Some insights:\n",
    "\n",
    "* Though `EfficientNet-B7` is the most accurate, the inference time for this model is large\n",
    "* `EfficientNet-B0` has a slightly lower accuracy compared to the larger models, but is able to perform inference about 10 times faster than `EfficientNet-B7`\n",
    "* `MobileNetV2` has a similar accuracy compared to the `VGG-16` and `VGG-19` models but has a slightly lower inference time\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2021/10/c4_pretrained_inf_speed_gpu.png' width=1000 align='center'><br/>\n",
    "\n",
    "Some insights:\n",
    "\n",
    "* When using a GPU, the infernce speed for all models is reduced significantly, but the greatest percentage reduction in inference speed is realized by the larger models. \n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvjpadPjE5k_"
   },
   "source": [
    "### 3.3 Parameter and Model Size Comparison\n",
    "\n",
    "* When we use a Deep Learning model on Edge devices, the model size is likely the deciding factor. At times, it is even more important than accuracy. \n",
    "* **MobileNetV3Small** has the smallest model size (`10 MB`), while **VGG-19** has the largest size (`549MB`).\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2021/10/c4_pretrained_para_size_compare.png' width=1000 align='center'><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqiMHHqaE5k_"
   },
   "source": [
    "### 3.4 MobileNet Model Comparison\n",
    "\n",
    "The following plot compares the accuracy of the four different **MobileNet** models vs inference speed. The comparison was performed on the Pixel 1 (CPU) phone. Although this phone was released in 2016 it still gives you an idea of the relative performance among the models.\n",
    "\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2021/10/c4_pretrained_mobilenet_compare.png' width=1000 align='center'><br/>\n",
    "\n",
    "* MobileNet models are a class of efficient models created for mobile and embedded vision applications. \n",
    "* These models have a streamlined architecture that uses **depthwise separable convolutions** to build light weight deep neural networks. \n",
    "* MobileNet models are specifically designed to perform inference fast with a small memory footprint, which is ideal for edge devices, but these attributes come at a cost with lower accuracy.\n",
    "* For their small size, these models perform quite well on ImageNet classification tasks compared to many other models that are quite a bit larger. \n",
    "\n",
    "**Note:** The bubble size corresponds to the model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5R7232QE5lA"
   },
   "source": [
    "### 3.5 Overall Comparison\n",
    "\n",
    "Let’s now combine all these important details in one bubble chart for easy reference (for the larger size model architectures). \n",
    "\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2021/10/c4_pretrained_overall_compare.png\" width=1000 align='center'><br/>\n",
    "\n",
    "\n",
    "**Note:**  The bubble size represents the relative CPU inference time. Annotations on the bubbles represent abbreviated model names and the numeric values indicate the CPU inference time in milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-1XEisYE5lA"
   },
   "source": [
    "## 4 Conclusion\n",
    "\n",
    "In this notebook, we learned how to easily use any of the pre-trained models in the Keras Applications API for classification tasks. We also made several comparisons between the models based on efficiency (memory footprint and inference speed) and performance (Top-1 and Top-5) accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "jNc8sMUd07xe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "c4_03_09_Pretrained_Models_for_Image_Classification_recorded.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": [],
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242.202px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
