{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gDI-clwHrxp"
   },
   "source": [
    "#  Classification Loss Functions\n",
    "\n",
    "In this notebook, we will explore the common loss functions used for classification tasks in Deep Learning. We will explore the following loss functions here:\n",
    "\n",
    "1. Binary Cross-Entropy\n",
    "2. Categorical Cross-Entropy\n",
    "3. Sparse Categorical Cross-Entropy\n",
    "\n",
    "We will begin with implementing the loss functions from scratch, then move on to see how they can be called using the `tf.keras.losses` module from TensorFlow.\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_02_binary_classification_loss.png' width=800 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koTTPstgRVd6"
   },
   "source": [
    "## Table of Contents\n",
    "* [1 Binary Cross Entropy Loss](#1-Binary-Cross-Entropy-Loss)\n",
    "* [2 Categorical Cross Entropy Loss](#2-Categorical-Cross-Entropy-Loss)\n",
    "* [3 Sparse Categorical Cross Entropy Loss](#3-Sparse-Categorical-Cross-Entropy-Loss)\n",
    "* [4 Implementing Loss Functions using tf.keras.losses Module](#4-Implementing-Loss-Functions-using-tf.keras.losses-Module)\n",
    "    * [4.1 Keras Binary Cross Entropy Loss](#4.1-Keras-Binary-Cross-Entropy-Loss)\n",
    "    * [4.2 Keras Categorical Cross Entropy Loss](#4.2-Keras-Categorical-Cross-Entropy-Loss)\n",
    "    * [4.3 Keras Sparse Categorical Cross Entropy Loss](#4.3-Keras-Sparse-Categorical-Cross-Entropy-Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "VgYcs0UjJUti"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "block_plot=False\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbVDgA3GJBtF"
   },
   "source": [
    "## 1 Binary Cross Entropy Loss\n",
    "We use the Binary Cross Entropy loss function while solving the binary classification problem in deep learning. In this case, the ground truth labels can either be 0 or 1.\n",
    "\n",
    "In a binary classification problem we wish to maximize the probability of the correct label (`y`).\n",
    "\n",
    "\\begin{equation}\n",
    "P(y|x) =\\left\\{\\begin{array}{cc} y{'} & y=1\\\\ 1-y{'} & y=0 \\end{array} \\right.\n",
    "\\end{equation}\n",
    "\n",
    "Where `y'` represents the predicted class label for the positive class (`y = 1`). And therefore, `(1 - y')` represents the probability of the negative class (`y = 0`).\n",
    "\n",
    "In simple words, the model outputs a single probability value for the predicted y. If the ground truth label is 1, then we take the predicted value as it is considering it as the probabilty for positive class. But if the corresponding ground truth label is 0, then we subtract the predicted value from 1 to get the probability of the positive class. This way we slowly try to maximize the probability for the correct label.\n",
    "\n",
    "The following the formula for Binary Cross Entropy Loss:\n",
    "$$\n",
    "J(y') = -y\\log(y') - (1-y)\\log(1-y')\n",
    "$$\n",
    "\n",
    "As we have only two classes here, we can also break the above equation in the following manner:\n",
    "\n",
    "\\begin{equation}\n",
    "J(y^{'})=\\left\\{\\begin{array}{cc} -\\log(y^{'}) & y=1\\\\ -\\log(1-y{'}) & y=0 \\end{array} \\right. \\label{eq2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wI5wAvj9YzJ"
   },
   "source": [
    "We can also plot the curve for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "collapsed": true,
    "id": "-vSas6cJ9b0J",
    "outputId": "05e97251-3a0e-404b-ffca-c5a0895912c6"
   },
   "outputs": [],
   "source": [
    "# Cross entropy plot y =1\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
    "x = tf.linspace(0, 1, 500)\n",
    "y1 = -tf.math.log(x)\n",
    "y0 = -tf.math.log(1 - x)\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, y1, color=\"b\", label=\"J(y') | y = 1\")\n",
    "plt.plot(x, y0, color=\"r\", label=\"J(y') | y = 0\")\n",
    "plt.xlabel(\"y'\")\n",
    "plt.ylabel(\"J(y')\")\n",
    "plt.legend(loc=\"upper center\")\n",
    "plt.title(\"Binary Cross-Entropy\")\n",
    "plt.show(block=block_plot)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_Kvraph9uLV"
   },
   "source": [
    "Let's see what is happening in the above curve.\n",
    "\n",
    "Supposing that a sample belongs to class `1` <font color=\"CornFlowerBlue\">(Blue Curve)</font>. If the output of the network is close to `0`, then from the above diagram we can see that the loss will be very high (The blue curve can go to infinity). Conversely, the loss is `0` if the output of the network is `1`. And we can apply a similar logic for the <font color=\"DarkOrange\">red curve</font>.\n",
    "\n",
    "So, for the binary classification, the error is high when the predicted output is different from the ground truth and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "G7QwY-F_JEx3"
   },
   "outputs": [],
   "source": [
    "def binary_crossentropy(y_true, y_pred):\n",
    "    # Convert ground truth and predictions to Tensor.\n",
    "    y_true = tf.convert_to_tensor(y_true)\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    loss = -(\n",
    "        (1.0 - y_true) * tf.math.log(1.0 - y_pred)) \\\n",
    "        - (y_true * tf.math.log(y_pred)\n",
    "    )\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aijuAvzirQvg"
   },
   "source": [
    "Let's start with an example, where the prediction is correct. Here, we take a simple example and try to classify whether the image is of a chair or not.\n",
    "Simply, it can be **chair (label 1)** or **not chair (label 0)**.\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_02_binary_classification_chair_example.jpg' width=500 align='center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "w46gRtbEJunt",
    "outputId": "44306b53-a01c-43a5-a193-b9ea4adf7c68"
   },
   "outputs": [],
   "source": [
    "# Ground truth of a sample where the label is positive.\n",
    "# Batch size is 1 and number of samples is 1.\n",
    "y_true = [[1.]]\n",
    "# The prediction close to 1 so the loss would be close to 0.\n",
    "y_pred = [[0.95]]\n",
    "bce_loss = binary_crossentropy(y_true, y_pred)\n",
    "print(bce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LPF57WCuEUx"
   },
   "source": [
    "In the above example, we can see that the loss is 0.051 which is close to 0. Now, let's take another example, where the grund truth label is 1 (indicating that is a **chair**), but the prediction is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "V9kRmYsyrNMn",
    "outputId": "b4ea4bd4-e25f-4df1-deb3-d55f448594eb"
   },
   "outputs": [],
   "source": [
    "# Ground truth of a sample where the label is positive.\n",
    "y_true = [[1.0]]\n",
    "# The prediction is much cloder to 0 this time.\n",
    "y_pred = [[0.05]]\n",
    "bce_loss = binary_crossentropy(y_true, y_pred)\n",
    "print(bce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgHMoAnKyHOc"
   },
   "source": [
    "As expected we get a very high loss compared to the previous case. This is becasue the model predicted the wrong result.\n",
    "\n",
    "For one final example, now, let's take a case where the ground truth label is 0, indicating that is not a chair. Also, here, the prediction is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "O5Z5DAEyy4OC",
    "outputId": "0ce3af3d-2436-4291-f12d-5f658fbbe84f"
   },
   "outputs": [],
   "source": [
    "# Ground truth of a sample where the label is positive.\n",
    "y_true = [[0.0]]\n",
    "# The prediction is 0.06\n",
    "y_pred = [[0.06]]\n",
    "bce_loss = binary_crossentropy(y_true, y_pred)\n",
    "print(bce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMPtQiwhzE_h"
   },
   "source": [
    "As we can see, when the ground truth label is 0 and model also predicted it correctly, in that case also, the loss will very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Crhk81CpJ-p-"
   },
   "source": [
    "## 2 Categorical Cross Entropy Loss\n",
    "Categorical Cross Entropy loss is used in the case of multi-class classification problem. Here, the index of the positive class in the ground truth labels is always 1.\n",
    "\n",
    "For example, let's say that we have three classes (class 0, class 1, class 2) and 2 samples. Assume that the first sample belongs to class 1. Then, the ground truth vector will be the following:\n",
    "\n",
    "`[0, 1.0, 0]`\n",
    "\n",
    "The index representing the ground truth class will be 1 and all else will be 0. This is also called an **one-hot encoded vector**.\n",
    "\n",
    "When the model predicts the values for each class, it will be between 0 and and 1 giving the probability of each class being correct. Let's consider that the model predicts the correct class for the sample. And we know that it belongs to class 1. So, in the output vector, the value at index 1 should have the highest value. Maybe something like the following:\n",
    "\n",
    "`[0.1, 0.8, 0.1]`\n",
    "\n",
    "The other two classes have lower scores than the true class.\n",
    "\n",
    "The following the formula for Categorical Cross-Entropy loss:\n",
    "\n",
    "$$\n",
    "Categorical \\ Cross \\ Entropy = -\\sum_{i=1}^{n}(p_i*log(q_i)), for \\ n \\ classes\n",
    "$$\n",
    "\n",
    "where $p$ is the vector containing the ground truth labels and $q$ is the vector containing the softmax probabilties of the output predictions. This brings to us the next detail we need to focus on. Here, we have multiple classes. And to calculate the loss, we sum over all the classes.\n",
    "\n",
    "In fact, we can also use Categorical Cross Entropy Loss function with label smoothing for the classes. The ground truths can be soft-labels instead of one-hot encoded labels. For example, [0.8, 0.1, 0.1] is an exmaple of a soft-label for a sample which belongs to class 0 and the dataset has a total of 3 classes.\n",
    "\n",
    "Although here, we will take a look at examples using one-hot encoded labels only.\n",
    "\n",
    "And the following function implements the Categorical Cross Entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "NIbKrfAwNMbP"
   },
   "outputs": [],
   "source": [
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    # Convert ground truth and predictions to Tensors.\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    loss = -(y_true*tf.math.log(y_pred))\n",
    "    flattened_loss = tf.reshape(loss, -1)\n",
    "    final_loss = [i for i in flattened_loss if i != 0]\n",
    "    final_loss = sum(final_loss)/len(final_loss)\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAIPJcjKGEFo"
   },
   "source": [
    "Now, we will try to replicate a few cases by taking a small example. Let's say that our three classes are **car (class 0)**, **truck (class 1)**, and **bike (class 2)**.\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_02_categorical_cross_entropy_example.jpg' width=550 align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b46CaM7GoqB"
   },
   "source": [
    "We will begin with an example where the sample belongs to class 0, that is car, and the predicted output is also correct. So, for the one-hot encoded ground truth vector, the value at first index will be 1, and all else will be 0. Similarly, for the predicted vector, the first index position value will have the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "-x5pI19eOS1l",
    "outputId": "45325e27-f1f7-4dc6-a9d7-3a5e1340d5d8"
   },
   "outputs": [],
   "source": [
    "y_true = [[1.0, 0, 0]]\n",
    "y_pred = [[0.90, 0.05, 0.05]]\n",
    "cce_loss = categorical_crossentropy(y_true, y_pred)\n",
    "print(cce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD2zBK7UIyMN"
   },
   "source": [
    "In this case, we got a loss of around 0.105. Hopefully, this is a good value when the prediction is correct. To confirm, let's take another example, where the prediction is wrong. If we get a higher loss, that would mean that the above loss is in the expected range when the prediction is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "SMAPmOoOJILC",
    "outputId": "ea00cd67-053b-4009-a505-9ac23d5cc68f"
   },
   "outputs": [],
   "source": [
    "# Sample belongs to class 0.\n",
    "y_true = [[1.0, 0, 0]]\n",
    "# Model predicted it as class 1.\n",
    "y_pred = [[0.025, 0.95, 0.025]]\n",
    "cce_loss = categorical_crossentropy(y_true, y_pred)\n",
    "print(cce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wTM4as3JRCq"
   },
   "source": [
    "Ok, this time with wrong prediction, is higher than the previous case. So, the previous loss for the correct prediction was in fact within the expected range.\n",
    "\n",
    "But what if the model predicts the correct class but only just with a bit more confidence than the other values. Meaning, the value at the correct class index will be highest but the other two values will also be nearer to that, In that case, most probably, the loss will be very near to 1. Maybe slightly more than or less than 1. Lets, try that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "GTyZNAnbKMl5",
    "outputId": "1b6b4d3b-e878-4d76-ad39-51f5cff3994e"
   },
   "outputs": [],
   "source": [
    "# Sample belongs to class 1.\n",
    "y_true = [[0, 1.0, 0]]\n",
    "# Prediction is correct, but model is not very confident.\n",
    "y_pred = [[0.3, 0.40, 0.3]]\n",
    "cce_loss = categorical_crossentropy(y_true, y_pred)\n",
    "print(cce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUX7INmRKzZC"
   },
   "source": [
    "We got a loss of around 0.91.\n",
    "\n",
    "We can get a sense of how the loss value gets impacted with the change in prediction. For one final example, let's check out the loss if the model predicts each class with equal confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "n2xVDFmlLJGd",
    "outputId": "5f3a1ee5-42b6-4cb1-eaa1-1e7179686f34"
   },
   "outputs": [],
   "source": [
    "# Sample belongs to class 1.\n",
    "y_true = [[0, 1.0, 0]]\n",
    "# The model predicted all classes with euqal confidence.\n",
    "y_pred = [[0.333, 0.333, 0.333]]\n",
    "cce_loss = categorical_crossentropy(y_true, y_pred)\n",
    "print(cce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hi2pmJeRJO1"
   },
   "source": [
    "This time, we get a value just near to 1.1. We can infer at least one thing from this. For the loss to be as low as possible, the predicted value for the correct class should be as high, and the predicted value for the other classes should be as low as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K-qR0PUtl20"
   },
   "source": [
    "## 3 Sparse Categorical Cross Entropy Loss\n",
    "For Sparse Categorical Cross Entropy loss, the format of the ground truth labels change a bit.\n",
    "\n",
    "Unlike Categorical Cross Entropy, here, the ground truth vector directly contains the class number. So, if we have three classes in total (0, 1, and 2) and two data points (samples), then the ground truth vector, where classes 1 and 2 are the true labels will be:\n",
    "\n",
    "`[1, 2]`\n",
    "\n",
    "We generally use Sparse Categorical Cross Entropy when the samples are mutually exclusive. This means that each sample can belong to one class. Therefore, we can represent the class it belongs to using just a single integer.\n",
    "\n",
    "So, the ground truth labels are simply integers instead of one-hot encoded vectors. The format of the predictions remain unchanged compared to Categorical Cross Entropy. **Also, this has a few benefits compared to Categorical Cross Entropy. When we have hundreds of classes, one-hot encoding takes more time and memory as we have encode for all the classes. But in the case of Sparse Categorical, we just have a single vector where each element specifies the class number for sample. Thus, conserving time and memory during calculation of loss.**\n",
    "\n",
    "The following code block shows the manual implementation of Sparse Categorical Cross Entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "id": "xL2u_zywtv0f"
   },
   "outputs": [],
   "source": [
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    num_samples = len(y_true)\n",
    "    loss = []\n",
    "    # Loop over the number of samples.\n",
    "    for i in range(num_samples):\n",
    "        y_pred_sample = y_pred[i] # The current prediction.\n",
    "        y_true_sample = y_true[i] # The current ground-truth.\n",
    "\n",
    "        # Loss for current iteration.\n",
    "        iter_loss = -tf.math.log(y_pred_sample[y_true_sample])\n",
    "\n",
    "        # Append sampe_loss to the loss list\n",
    "        loss.append(iter_loss)\n",
    "\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arJvACDqRmKO"
   },
   "source": [
    "Here, let's try to replicate the cases we had in Categorical Cross Entropy. Starting with when class 0 is the ground truth for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Q0a4fPvnRwIK",
    "outputId": "d989d170-2ea9-40d2-ea63-bf23c159c1e9"
   },
   "outputs": [],
   "source": [
    "y_true = [0]\n",
    "y_pred = [[0.90, 0.05, 0.05]]\n",
    "scce_loss = sparse_categorical_crossentropy(y_true, y_pred)\n",
    "print(scce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP4GUJdzSn0m"
   },
   "source": [
    "Now, when the sample belongs to class 0, but prediction is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "FCri-lzUSu3l",
    "outputId": "b273aa45-375d-43f8-f208-709b27c30ead"
   },
   "outputs": [],
   "source": [
    "# Sample belongs to class 0.\n",
    "y_true = [0]\n",
    "# Model predicted it as class 1.\n",
    "y_pred = [[0.025, 0.95, 0.025]]\n",
    "scce_loss = sparse_categorical_crossentropy(y_true, y_pred)\n",
    "print(scce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siDcngmwTA5W"
   },
   "source": [
    "Moving on to when sample belongs to class 1, the prediction is correct, but model not too confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BQpbqs2WTEaJ",
    "outputId": "1fc53520-9aa1-4df1-b5bd-06575f046493"
   },
   "outputs": [],
   "source": [
    "# Sample belongs to class 1.\n",
    "y_true = [1]\n",
    "# Prediction is correct, but model is not very confident.\n",
    "y_pred = [[0.3, 0.40, 0.3]]\n",
    "scce_loss = sparse_categorical_crossentropy(y_true, y_pred)\n",
    "print(scce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FjQJ3fsTYOI"
   },
   "source": [
    "Finally, when model predicts everything with equal confidence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "YhEh7HqETeVF",
    "outputId": "0c9d9927-7174-4ebe-dccd-6db6eb2a0036"
   },
   "outputs": [],
   "source": [
    "# Sample belongs to class 1.\n",
    "y_true = [1]\n",
    "# Prediction is correct, but model is not very confident.\n",
    "y_pred = [[0.333, 0.333, 0.333]]\n",
    "scce_loss = sparse_categorical_crossentropy(y_true, y_pred)\n",
    "print(scce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7gw2p6I44Pr"
   },
   "source": [
    "As you can see, for the same set of ground truth and predicted values (in their respective correct format), both, Categorcal Cross Entropy and Sparse Categorical Cross Entropy give the same output. This is expected and also a good way to check the manual implementation of out loss fuctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqM_YtzCCo_x"
   },
   "source": [
    "## 4 Implementing Loss Functions using tf.keras.losses Module\n",
    "In the previous section, we saw how to implement the common classification loss functions manually using the `tf.math` modules and functions.\n",
    "\n",
    "But TensorFlow provides the `tf.keras.losses` module which already has everything defined for us. Now, let's cross-check using the `tf.keras.losses` whether all our implementation are correct or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9nd4IyWDANc"
   },
   "source": [
    "### 4.1 Keras Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "hdNLugUGDyqb",
    "outputId": "7ab9360b-97f1-4df5-83da-bb395bde99a8"
   },
   "outputs": [],
   "source": [
    "y_true = [[1.]]\n",
    "y_pred = [[0.95]]\n",
    "\n",
    "y_true = tf.convert_to_tensor(y_true)\n",
    "y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "# Binary Cross Entropy\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce_loss = bce(y_true, y_pred)\n",
    "print(bce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LspF7mBaD9TK"
   },
   "source": [
    "### 4.2 Keras Categorical Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvFvD2gUbcU6"
   },
   "source": [
    "You might have noticed the `from_logits` argument that we are passing to the loss functions here. It is quite important to know that as well. There are two possible values we can pass to argument:\n",
    "* `from_logits=False`: This indicates that the output of the neural network are already passed through an activation function. So, when calling the Keras loss function, it will not apply any action to the given prediction values.This is mainly the case we apply either the `sigmoid` or `softmax` activation to the last layer of the neural network.\n",
    "* `from_logits=True`: This indicates that the prediction values that we pass to the Keras loss functions are raw outputs from the neural network and the last layer of the neural network did not contain any activation function.\n",
    "\n",
    "In either case, we should get the same results. Let's check that out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "TE1Ht0KOdDu5",
    "outputId": "c31f36be-b586-41bb-899d-ce82dc86aba2"
   },
   "outputs": [],
   "source": [
    "# Prediction values replicate logits from the last layer of the neural network.\n",
    "y_true = [[1.0, 0, 0]]\n",
    "y_pred = [[12., 5., 7.]]\n",
    "\n",
    "y_true = tf.convert_to_tensor(y_true)\n",
    "y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "# Categorical Cross Entropy\n",
    "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "cce_loss = cce(y_true, y_pred)\n",
    "print(cce_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "scpl8fFrdbUb",
    "outputId": "4107ca4b-4c56-47af-88b7-fa260e306947"
   },
   "outputs": [],
   "source": [
    "y_true = [[1.0, 0, 0]]\n",
    "# The following are the softmax prediction values of [[12., 5., 7.]]\n",
    "y_pred = [[0.99240828, 0.00090495922, 0.0066867946]]\n",
    "\n",
    "y_true = tf.convert_to_tensor(y_true)\n",
    "y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "# Categorical Cross Entropy\n",
    "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "cce_loss = cce(y_true, y_pred)\n",
    "print(cce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu5DB0bId79s"
   },
   "source": [
    "As you can see, in both cases we have the same loss value except the last two decimal places which will be pretty insignificant in a real-world use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4h71cG4-ELoh",
    "outputId": "367da94b-4fd5-4aa7-c3be-dc08b97bb0f8"
   },
   "outputs": [],
   "source": [
    "y_true = [[1.0, 0, 0]]\n",
    "y_pred = [[0.90, 0.05, 0.05]]\n",
    "\n",
    "y_true = tf.convert_to_tensor(y_true)\n",
    "y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "# Categorical Cross Entropy\n",
    "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "cce_loss = cce(y_true, y_pred)\n",
    "print(cce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWRkD_RAEoG-"
   },
   "source": [
    "### 4.3 Keras Sparse Categorical Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "JM-hvhFvEc6Y",
    "outputId": "c5399c2a-865d-40b5-ef06-00b302c16c68"
   },
   "outputs": [],
   "source": [
    "y_true = [0]\n",
    "y_pred = [[0.90, 0.05, 0.05]]\n",
    "\n",
    "y_true = tf.convert_to_tensor(y_true)\n",
    "y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "\n",
    "# Sparse Categorical Cross Entropy\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "scce_loss = scce(y_true, y_pred)\n",
    "print(scce_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCpjweQkEvKr"
   },
   "source": [
    "All the loss values from the `tf.keras.losses` modules match the ones from our manual implementation. But you can see how easily we can invoke the Keras module loss functions and get the answers with just one line of code. For large scale problems, it is always advisable to use the predefined loss functions to avoid unnecesarry confusion and errors."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": [],
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325.198px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "415.828px",
    "left": "909.036px",
    "right": "20px",
    "top": "98.9838px",
    "width": "326.98px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
